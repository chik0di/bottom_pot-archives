{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Website](https://nigeriapropertycentre.com/for-sale/houses/showtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing necessary Libraries\n",
    "- requests to send HTTP/1.1 requests easily, such as GET and POST, to interact with web APIs or retrieve web pages\n",
    "\n",
    "- pandas for data handling, cleaning, manipulation and analysis.\n",
    "- BeautifulSoup for parsing HTML and XML documents to extract specific data elements using a tree-like structure.\n",
    "- selenium automates web browsers and user interaction like clicking buttons or dynamically waiting for items to load. Also for scraping sites that requires JavaScript rendering.\n",
    "    - Service manages the ChromeDriver service for Selenium to interact with the Chrome browse\n",
    "\n",
    "    - By provides methods to locate elements on a webpage (e.g., by ID, name, class name, etc.).\n",
    "- time provides time-related functions like adding delays (e.g., time.sleep()), and working with timestamps, or measuring execution time.\n",
    "- tqdm for visualizing the progress of loops in data processing or web scraping.\n",
    "- json for serializing Python objects into JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“„ Full Scraping Run (Pages to Properties)\n",
    "\n",
    "The below cell:\n",
    "\n",
    "- Visits each page in the specified range (_start_page_ to _end_page_)\n",
    "\n",
    "- Collects all property listing links\n",
    "\n",
    "- Iterates through each link, scrapes property details, and saves them to _crib.json_\n",
    "\n",
    "If interrupted, I can resume scraping later using the saved links â€” see the next cell for how I did that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:/Users/HP/Downloads/chromedriver-win64/chromedriver.exe\"\n",
    "service = Service(path)\n",
    "\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "total_properties = 0 \n",
    "property_links_all_pages = [] \n",
    "\n",
    "destination = \"crib.json\" \n",
    "if not os.path.exists(destination):\n",
    "    with open(destination, 'w') as f:\n",
    "        json.dump([], f)\n",
    "\n",
    "first_iteration = True\n",
    "start_page = 419\n",
    "end_page = 430\n",
    "\n",
    "for x in range(start_page, end_page):\n",
    "    \n",
    "    url = f\"https://nigeriapropertycentre.com/for-sale/houses/showtype?page={x}\"\n",
    "    driver.get(url)\n",
    "\n",
    "    property_links = [link.get_attribute('href') for link in driver.find_elements(By.XPATH, '//a[@itemprop=\"url\"]')]\n",
    "    property_links_all_pages.extend(property_links) \n",
    "\n",
    "with tqdm(total=len(property_links_all_pages), desc=\"Scraping properties\") as pbar:\n",
    "    for link_url in property_links_all_pages:\n",
    "        driver.get(link_url)\n",
    "        properties = []\n",
    "\n",
    "        time.sleep(10)\n",
    "\n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "        rows = soup.find('table', class_= 'table table-bordered table-striped')\n",
    "\n",
    "        try:\n",
    "            for row in rows:\n",
    "                line = row.find_all('td')\n",
    "                fields = [h.text.strip() for h in line]\n",
    "                \n",
    "\n",
    "                table = {\n",
    "                    field.split(\":\")[0].strip(): field.split(\":\")[1].strip() \n",
    "                    for field in fields if \":\" in field # and len(field.split(\":\")) > 1\n",
    "                    }\n",
    "                try:\n",
    "                    figure = soup.find_all('span', class_='price')[1]\n",
    "                    price = float(figure.attrs['content'])\n",
    "                    table['Price'] = price\n",
    "                except:\n",
    "                    pass \n",
    "                try: \n",
    "                    dollar = soup.find('span', class_='naira-equiv')\n",
    "                    equiv = float(dollar.text.split()[1].replace(',','')[1:])           \n",
    "                    table['Price'] = equiv\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "                address = soup.find('div', class_='col-sm-8 f15 property-details')\n",
    "                location = address.text.strip().split(',')[-2:]\n",
    "\n",
    "                if '\\n \\xa0'in location[0]:\n",
    "                    city = location[0].split('\\n \\xa0')\n",
    "                    district = city[1].strip()\n",
    "                else:\n",
    "                    district = location[0].strip()\n",
    "\n",
    "                state = location[1].strip()\n",
    "\n",
    "                table['District'] = district \n",
    "                table['State'] = state\n",
    "\n",
    "                with open(destination, 'r') as f:\n",
    "                    content = f.read()\n",
    "                    data = json.loads(content) if content.strip() else []\n",
    "                data.append(table)\n",
    "\n",
    "                with open(destination, 'w') as f:\n",
    "                    json.dump(data, f, indent=4) \n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f'Error with property: {link_url} - {e}')\n",
    "        \n",
    "        total_properties +=1\n",
    "        pbar.update(1)\n",
    "\n",
    "            # back = driver.find_element(By.XPATH, '//a[@class=\"underline\"]')\n",
    "            # back.click()\n",
    "        # driver.back()                                                                                                                                                      \n",
    "        time.sleep(2) \n",
    "\n",
    "driver.quit()\n",
    "print(f\"Successfully scraped {len(property_links_all_pages)} properties across {end_page - start_page} pages.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”„ Resuming Scraping After Interruption\n",
    "\n",
    "If scraping is interrupted before all properties are processed (e.g., due to a network or WebDriver error), the cell below allows me to resume from where the process stopped.\n",
    "\n",
    "I reuse the previously collected list of property links and simply iterate from the index of the last successfully scraped property to the end of the list â€” avoiding duplicates and saving time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:/Users/HP/Downloads/chromedriver-win64/chromedriver.exe\"\n",
    "service = Service(path)\n",
    "\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "remaining_properties = 0\n",
    "\n",
    "with tqdm(total=len(property_links_all_pages[69:]), desc=\"Scraping properties\") as pbar:\n",
    "    for link_url in property_links_all_pages[69:]:\n",
    "        driver.get(link_url)\n",
    "        properties = []\n",
    "\n",
    "        time.sleep(10)\n",
    "\n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "        rows = soup.find('table', class_= 'table table-bordered table-striped')\n",
    "\n",
    "        try:\n",
    "            for row in rows:\n",
    "                line = row.find_all('td')\n",
    "                fields = [h.text.strip() for h in line]\n",
    "                \n",
    "\n",
    "                table = {\n",
    "                    field.split(\":\")[0].strip(): field.split(\":\")[1].strip() \n",
    "                    for field in fields if \":\" in field # and len(field.split(\":\")) > 1\n",
    "                    }\n",
    "                try:\n",
    "                    figure = soup.find_all('span', class_='price')[1]\n",
    "                    price = float(figure.attrs['content'])\n",
    "                    table['Price'] = price\n",
    "                except:\n",
    "                    pass \n",
    "                try: \n",
    "                    dollar = soup.find('span', class_='naira-equiv')\n",
    "                    equiv = float(dollar.text.split()[1].replace(',','')[1:])           \n",
    "                    table['Price'] = equiv\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "                address = soup.find('div', class_='col-sm-8 f15 property-details')\n",
    "                location = address.text.strip().split(',')[-2:]\n",
    "\n",
    "                if '\\n \\xa0'in location[0]:\n",
    "                    city = location[0].split('\\n \\xa0')\n",
    "                    district = city[1].strip()\n",
    "                else:\n",
    "                    district = location[0].strip()\n",
    "\n",
    "                state = location[1].strip()\n",
    "\n",
    "                table['District'] = district \n",
    "                table['State'] = state\n",
    "\n",
    "                with open(destination, 'r') as f:\n",
    "                    content = f.read()\n",
    "                    data = json.loads(content) if content.strip() else []\n",
    "                data.append(table)\n",
    "\n",
    "                with open(destination, 'w') as f:\n",
    "                    json.dump(data, f, indent=4) \n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f'Error with property: {link_url} - {e}')\n",
    "        \n",
    "        remaining_properties +=1\n",
    "        pbar.update(1)                                                                                                                                                 \n",
    "        time.sleep(2) \n",
    "\n",
    "driver.quit()\n",
    "print(f\"Successfully scraped the remaining {len(property_links_all_pages[69:])} properties from the entire {property_links_all_pages} properties.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully converted JSON to CSV. The CSV is saved as crib.csv\n"
     ]
    }
   ],
   "source": [
    "final_destination = \"crib.csv\"\n",
    "\n",
    "with open(destination, 'r') as f:\n",
    "    properties = json.load(f)\n",
    "\n",
    "df = pd.DataFrame(properties)\n",
    "\n",
    "df.to_csv(final_destination, index=False)\n",
    "\n",
    "print(f\"Successfully converted JSON to CSV. The CSV is saved as {final_destination}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
