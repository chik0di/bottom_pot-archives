{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Website](https://nigeriapropertycentre.com/for-sale/houses/showtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import Select\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single Page Property Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:/Users/HP/Downloads/chromedriver-win64/chromedriver.exe\"\n",
    "url = \"https://nigeriapropertycentre.com/for-sale/houses/showtype?page=1\"\n",
    "\n",
    "service = Service(path)\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "driver.get(url)\n",
    "property_links = [link.get_attribute('href') for link in driver.find_elements(By.XPATH, '//a[@itemprop=\"url\"]')]\n",
    "\n",
    "properties = []\n",
    "\n",
    "for link_url in property_links:\n",
    "    driver.get(link_url)\n",
    "\n",
    "    time.sleep(3)\n",
    "\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, 'html')\n",
    "\n",
    "    rows = soup.find('table', class_= 'table table-bordered table-striped')\n",
    "\n",
    "    \n",
    "    for row in rows:\n",
    "        line = row.find_all('td')\n",
    "        fields = [h.text.strip() for h in line]\n",
    "        \n",
    "\n",
    "        table = {\n",
    "            field.split(\":\")[0].strip(): field.split(\":\")[1].strip() \n",
    "            for field in fields if \":\" in field # and len(field.split(\":\")) > 1\n",
    "            }\n",
    "        try:\n",
    "            figure = soup.find_all('span', class_='price')[1]\n",
    "            price = float(figure.attrs['content'])\n",
    "            table['Price'] = price\n",
    "        except:\n",
    "            pass \n",
    "        try: \n",
    "            dollar = soup.find('span', class_='naira-equiv')\n",
    "            equiv = float(dollar.text.split()[1].replace(',','')[1:])           \n",
    "            table['Price'] = equiv\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        address = soup.find('div', class_='col-sm-8 f15 property-details')\n",
    "        location = address.text.strip().split(',')[-2:]\n",
    "        city = location[0].strip()\n",
    "        state = location[1].strip()\n",
    "        table['District'] = city \n",
    "        table['State'] = state\n",
    "\n",
    "        properties.append(table)\n",
    "\n",
    "        # back = driver.find_element(By.XPATH, '//a[@class=\"underline\"]')\n",
    "        # back.click()\n",
    "    # driver.back()\n",
    "    time.sleep(2) \n",
    "\n",
    "# driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traditional Pagination Strategy for Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties = []\n",
    "\n",
    "for x in range(1,201):\n",
    "    \n",
    "    path = \"C:/Users/HP/Downloads/chromedriver-win64/chromedriver.exe\"\n",
    "    \n",
    "    url = f\"https://nigeriapropertycentre.com/for-sale/houses/showtype?page={x}\"\n",
    "\n",
    "    service = Service(path)\n",
    "    driver = webdriver.Chrome(service=service)\n",
    "\n",
    "    driver.get(url)\n",
    "\n",
    "    property_links = [link.get_attribute('href') for link in driver.find_elements(By.XPATH, '//a[@itemprop=\"url\"]')]\n",
    "\n",
    "    for link_url in property_links:\n",
    "        driver.get(link_url)\n",
    "\n",
    "        time.sleep(10)\n",
    "\n",
    "        # try:\n",
    "        #     WebDriverWait(driver,10).until(\n",
    "        #         EC.visibility_of_element_located(By.XPATH, '//table[@class=\"table table-bordered table-striped\"]')\n",
    "        #     )\n",
    "        # except Exception as e:\n",
    "        #     print(f\"Error waiting for table on {link_url}: {e}\")\n",
    "        #     continue\n",
    "\n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'html')\n",
    "\n",
    "        rows = soup.find('table', class_= 'table table-bordered table-striped')\n",
    "\n",
    "        \n",
    "        for row in rows:\n",
    "            line = row.find_all('td')\n",
    "            fields = [h.text.strip() for h in line]\n",
    "            \n",
    "\n",
    "            table = {\n",
    "                field.split(\":\")[0].strip(): field.split(\":\")[1].strip() \n",
    "                for field in fields if \":\" in field # and len(field.split(\":\")) > 1\n",
    "                }\n",
    "            try:\n",
    "                figure = soup.find_all('span', class_='price')[1]\n",
    "                price = float(figure.attrs['content'])\n",
    "                table['Price'] = price\n",
    "            except:\n",
    "                pass \n",
    "            try: \n",
    "                dollar = soup.find('span', class_='naira-equiv')\n",
    "                equiv = float(dollar.text.split()[1].replace(',','')[1:])           \n",
    "                table['Price'] = equiv\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            address = soup.find('div', class_='col-sm-8 f15 property-details')\n",
    "            location = address.text.strip().split()[-2:]\n",
    "            city = location[0].replace(',','')\n",
    "            state = location[1].strip()\n",
    "            table['District'] = city \n",
    "            table['State'] = state\n",
    "            \n",
    "            properties.append(table)\n",
    "\n",
    "            # back = driver.find_element(By.XPATH, '//a[@class=\"underline\"]')\n",
    "            # back.click()\n",
    "        # driver.back()\n",
    "        time.sleep(2) \n",
    "\n",
    "# driver.quit()\n",
    "# print(f'Successfully scraped {x} pages')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempting to involve tqdm to help show the progress bar of pages scraped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Service' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m service \u001b[38;5;241m=\u001b[39m \u001b[43mService\u001b[49m(path)\n\u001b[0;32m      2\u001b[0m driver \u001b[38;5;241m=\u001b[39m webdriver\u001b[38;5;241m.\u001b[39mChrome(service\u001b[38;5;241m=\u001b[39mservice)\n\u001b[0;32m      3\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/HP/Downloads/chromedriver-win64/chromedriver.exe\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Service' is not defined"
     ]
    }
   ],
   "source": [
    "service = Service(path)\n",
    "driver = webdriver.Chrome(service=service)\n",
    "path = \"C:/Users/HP/Downloads/chromedriver-win64/chromedriver.exe\"\n",
    "\n",
    "properties = []\n",
    "\n",
    "start_page = 1\n",
    "end_page = 50\n",
    "\n",
    "for x in tqdm(range(start_page, end_page), desc=\"Scraping Pages\"):\n",
    "    \n",
    "    url = f\"https://nigeriapropertycentre.com/for-sale/houses/showtype?page={x}\"\n",
    "    driver.get(url)\n",
    "\n",
    "    property_links = [link.get_attribute('href') for link in driver.find_elements(By.XPATH, '//a[@itemprop=\"url\"]')]\n",
    "\n",
    "    for link_url in property_links:\n",
    "        driver.get(link_url)\n",
    "\n",
    "        time.sleep(10)\n",
    "\n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'html')\n",
    "\n",
    "        rows = soup.find('table', class_= 'table table-bordered table-striped')\n",
    "\n",
    "        try:\n",
    "            for row in rows:\n",
    "                line = row.find_all('td')\n",
    "                fields = [h.text.strip() for h in line]\n",
    "                \n",
    "\n",
    "                table = {\n",
    "                    field.split(\":\")[0].strip(): field.split(\":\")[1].strip() \n",
    "                    for field in fields if \":\" in field # and len(field.split(\":\")) > 1\n",
    "                    }\n",
    "                try:\n",
    "                    figure = soup.find_all('span', class_='price')[1]\n",
    "                    price = float(figure.attrs['content'])\n",
    "                    table['Price'] = price\n",
    "                except:\n",
    "                    pass \n",
    "                try: \n",
    "                    dollar = soup.find('span', class_='naira-equiv')\n",
    "                    equiv = float(dollar.text.split()[1].replace(',','')[1:])           \n",
    "                    table['Price'] = equiv\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "                address = soup.find('div', class_='col-sm-8 f15 property-details')\n",
    "                location = address.text.strip().split()[-2:]\n",
    "                city = location[0].replace(',','')\n",
    "                state = location[1].strip()\n",
    "                table['District'] = city \n",
    "                table['State'] = state\n",
    "\n",
    "                properties.append(table)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {link_url}: {e}\")\n",
    "            \n",
    "        time.sleep(2) \n",
    "\n",
    "# driver.quit()\n",
    "print(f\"Successfully scraped {len(properties)} properties across {end_page - start_page} pages.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service = Service(path)\n",
    "driver = webdriver.Chrome(service=service)\n",
    "path = \"C:/Users/HP/Downloads/chromedriver-win64/chromedriver.exe\"\n",
    "\n",
    "properties = []\n",
    "\n",
    "total_properties = 0 \n",
    "property_links_all_pages = []\n",
    "\n",
    "start_page = 1\n",
    "end_page = 250\n",
    "\n",
    "for x in range(start_page, end_page):\n",
    "    \n",
    "    url = f\"https://nigeriapropertycentre.com/for-sale/houses/showtype?page={x}\"\n",
    "    driver.get(url)\n",
    "\n",
    "    property_links = [link.get_attribute('href') for link in driver.find_elements(By.XPATH, '//a[@itemprop=\"url\"]')]\n",
    "    property_links_all_pages.extend(property_links) \n",
    "\n",
    "with tqdm(total=len(property_links_all_pages), desc=\"Scraping properties\") as pbar:\n",
    "    for link_url in property_links_all_pages:\n",
    "        driver.get(link_url)\n",
    "\n",
    "        time.sleep(10)\n",
    "\n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'html')\n",
    "\n",
    "        rows = soup.find('table', class_= 'table table-bordered table-striped')\n",
    "\n",
    "        \n",
    "        for row in rows:\n",
    "            line = row.find_all('td')\n",
    "            fields = [h.text.strip() for h in line]\n",
    "            \n",
    "\n",
    "            table = {\n",
    "                field.split(\":\")[0].strip(): field.split(\":\")[1].strip() \n",
    "                for field in fields if \":\" in field # and len(field.split(\":\")) > 1\n",
    "                }\n",
    "            try:\n",
    "                figure = soup.find_all('span', class_='price')[1]\n",
    "                price = float(figure.attrs['content'])\n",
    "                table['Price'] = price\n",
    "            except:\n",
    "                pass \n",
    "            try: \n",
    "                dollar = soup.find('span', class_='naira-equiv')\n",
    "                equiv = float(dollar.text.split()[1].replace(',','')[1:])           \n",
    "                table['Price'] = equiv\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            address = soup.find('div', class_='col-sm-8 f15 property-details')\n",
    "            location = address.text.strip().split()[-2:]\n",
    "            city = location[0].replace(',','')\n",
    "            state = location[1].strip()\n",
    "            table['District'] = city \n",
    "            table['State'] = state\n",
    "\n",
    "            properties.append(table)\n",
    "\n",
    "            total_properties +=1\n",
    "            pbar.update(1)\n",
    "\n",
    "            # back = driver.find_element(By.XPATH, '//a[@class=\"underline\"]')\n",
    "            # back.click()\n",
    "        # driver.back()\n",
    "        time.sleep(2) \n",
    "\n",
    "# driver.quit()\n",
    "print(f\"Successfully scraped {len(properties)} properties across {end_page - start_page} pages.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Selenium to scrape table containing property info for a particular property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ikoyi\n"
     ]
    }
   ],
   "source": [
    "# path = \"C:/Users/HP/Downloads/chromedriver-win64/chromedriver.exe\"\n",
    "url = \"https://nigeriapropertycentre.com/for-sale/houses/semi-detached-duplexes/lagos/ikoyi/2598042-lovely-5bed-semi-detached-home-with-a-bq\"\n",
    "\n",
    "# service = Service(path)\n",
    "# driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# driver.get(url)\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "# page_source = driver.page_source\n",
    "soup = BeautifulSoup(response.text, 'html')\n",
    "\n",
    "address = soup.find('div', class_='col-sm-8 f15 property-details')\n",
    "location = address.text.strip().split()[-2:]\n",
    "city = location[0].replace(',','')\n",
    "\n",
    "print(city)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
