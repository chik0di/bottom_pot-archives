{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Website](https://nigeriapropertycentre.com/for-sale/houses/showtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing necessary Libraries\n",
    "- requests to send HTTP/1.1 requests easily, such as GET and POST, to interact with web APIs or retrieve web pages\n",
    "\n",
    "- pandas for data handling, cleaning, manipulation and analysis.\n",
    "- BeautifulSoup for parsing HTML and XML documents to extract specific data elements using a tree-like structure.\n",
    "- selenium automates web browsers and user interaction like clicking buttons or dynamically waiting for items to load. Also for scraping sites that requires JavaScript rendering.\n",
    "    - Service manages the ChromeDriver service for Selenium to interact with the Chrome browse\n",
    "\n",
    "    - By provides methods to locate elements on a webpage (e.g., by ID, name, class name, etc.).\n",
    "- time provides time-related functions like adding delays (e.g., time.sleep()), and working with timestamps, or measuring execution time.\n",
    "- tqdm for visualizing the progress of loops in data processing or web scraping.\n",
    "- json for serializing Python objects into JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Real Estate Data\n",
    "\n",
    "This page contains over 40000 properties so the scraping logic is basically: \n",
    "- iterate through between 100 and 200 pages at once (each page contains 10 properties)\n",
    "- each iteration collects the link to each property on that page \n",
    "- when page iteration is concluded, property iteration begins directly through each property\n",
    "- adjust the start_page and end_page variables to scrape other pages as required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping properties: 100%|██████████| 367/367 [1:31:11<00:00, 14.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped 367 properties across 18 pages.\n"
     ]
    }
   ],
   "source": [
    "path = \"C:/Users/HP/Downloads/chromedriver-win64/chromedriver.exe\"\n",
    "service = Service(path)\n",
    "\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "total_properties = 0 \n",
    "property_links_all_pages = [] \n",
    "\n",
    "destination = \"crib.json\" \n",
    "if not os.path.exists(destination):\n",
    "    with open(destination, 'w') as f:\n",
    "        json.dump([], f)\n",
    "\n",
    "first_iteration = True\n",
    "start_page = 282\n",
    "end_page = 300\n",
    "\n",
    "for x in range(start_page, end_page):\n",
    "    \n",
    "    url = f\"https://nigeriapropertycentre.com/for-sale/houses/showtype?page={x}\"\n",
    "    driver.get(url)\n",
    "\n",
    "    property_links = [link.get_attribute('href') for link in driver.find_elements(By.XPATH, '//a[@itemprop=\"url\"]')]\n",
    "    property_links_all_pages.extend(property_links) \n",
    "\n",
    "with tqdm(total=len(property_links_all_pages), desc=\"Scraping properties\") as pbar:\n",
    "    for link_url in property_links_all_pages:\n",
    "        driver.get(link_url)\n",
    "        properties = []\n",
    "\n",
    "        time.sleep(10)\n",
    "\n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "        rows = soup.find('table', class_= 'table table-bordered table-striped')\n",
    "\n",
    "        try:\n",
    "            for row in rows:\n",
    "                line = row.find_all('td')\n",
    "                fields = [h.text.strip() for h in line]\n",
    "                \n",
    "\n",
    "                table = {\n",
    "                    field.split(\":\")[0].strip(): field.split(\":\")[1].strip() \n",
    "                    for field in fields if \":\" in field # and len(field.split(\":\")) > 1\n",
    "                    }\n",
    "                try:\n",
    "                    figure = soup.find_all('span', class_='price')[1]\n",
    "                    price = float(figure.attrs['content'])\n",
    "                    table['Price'] = price\n",
    "                except:\n",
    "                    pass \n",
    "                try: \n",
    "                    dollar = soup.find('span', class_='naira-equiv')\n",
    "                    equiv = float(dollar.text.split()[1].replace(',','')[1:])           \n",
    "                    table['Price'] = equiv\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "                address = soup.find('div', class_='col-sm-8 f15 property-details')\n",
    "                location = address.text.strip().split(',')[-2:]\n",
    "\n",
    "                if '\\n \\xa0'in location[0]:\n",
    "                    city = location[0].split('\\n \\xa0')\n",
    "                    district = city[1].strip()\n",
    "                else:\n",
    "                    district = location[0].strip()\n",
    "\n",
    "                state = location[1].strip()\n",
    "\n",
    "                table['District'] = district \n",
    "                table['State'] = state\n",
    "\n",
    "                with open(destination, 'r') as f:\n",
    "                    content = f.read()\n",
    "                    data = json.loads(content) if content.strip() else []\n",
    "                data.append(table)\n",
    "\n",
    "                with open(destination, 'w') as f:\n",
    "                    json.dump(data, f, indent=4) \n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f'Error with property: {link_url} - {e}')\n",
    "        \n",
    "        total_properties +=1\n",
    "        pbar.update(1)\n",
    "\n",
    "            # back = driver.find_element(By.XPATH, '//a[@class=\"underline\"]')\n",
    "            # back.click()\n",
    "        # driver.back()                                                                                                                                                      \n",
    "        time.sleep(2) \n",
    "\n",
    "driver.quit()\n",
    "print(f\"Successfully scraped {len(property_links_all_pages)} properties across {end_page - start_page} pages.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6282"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully converted JSON to CSV. The CSV is saved as crib.csv\n"
     ]
    }
   ],
   "source": [
    "final_destination = \"crib.csv\"\n",
    "\n",
    "with open(destination, 'r') as f:\n",
    "    properties = json.load(f)\n",
    "\n",
    "df = pd.DataFrame(properties)\n",
    "\n",
    "df.to_csv(final_destination, index=False)\n",
    "\n",
    "print(f\"Successfully converted JSON to CSV. The CSV is saved as {final_destination}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
